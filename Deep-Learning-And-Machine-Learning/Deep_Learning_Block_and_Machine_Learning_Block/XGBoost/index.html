<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the Gradient Boosting framework."><meta property="og:title" content="XGBoost"><meta property="og:description" content="XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the Gradient Boosting framework."><meta property="og:type" content="website"><meta property="og:image" content="https://pinktalk.online/icon.png"><meta property="og:url" content="https://pinktalk.online/Deep-Learning-And-Machine-Learning/Deep_Learning_Block_and_Machine_Learning_Block/XGBoost/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="XGBoost"><meta name=twitter:description content="XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the Gradient Boosting framework."><meta name=twitter:image content="https://pinktalk.online/icon.png"><meta name=twitter:site content="PinkR1ver"><title>XGBoost</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://pinktalk.online//icon.png><link href=https://pinktalk.online/styles.a14d532e479de9b6c94ea08c8c9fdc07.min.css rel=stylesheet><link href=https://pinktalk.online/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://pinktalk.online/js/darkmode.4b1d856da524d844cfa5256b4795119b.min.js></script>
<script src=https://pinktalk.online/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://pinktalk.online/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pinktalk.online/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pinktalk.online/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://pinktalk.online/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pinktalk.online/",fetchData=Promise.all([fetch("https://pinktalk.online/indices/linkIndex.706399597e877c9d59b879cc82735496.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pinktalk.online/indices/contentIndex.94ce5b9637943674c358522414109471.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://pinktalk.online",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pinktalk.online",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pinktalk.online\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pinktalk.online src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pinktalk.online/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://pinktalk.online/>JudeW's Digital Garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>XGBoost</h1><p class=meta>Last updated
Apr 13, 2023
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/XGBoost.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://pinktalk.online/tags/deep-learning/>Deep learning</a></li><li><a href=https://pinktalk.online/tags/ensemble-learning/>Ensemble learning</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#what-you-need-to-know-first>What you need to know first</a></li><li><a href=#what-is-xgboost>What is XGBoost</a><ol><li><a href=#what-is-ensemble-learning集成学习>What is Ensemble Learning(集成学习)</a><ol><li><a href=#bagging>Bagging</a></li><li><a href=#stacking>Stacking</a></li><li><a href=#boosting>Boosting</a></li></ol></li></ol></li><li><a href=#introduction-to-three-main-type-of-boosting-method>Introduction to three main type of boosting method</a><ol><li><a href=#adaptive-boostinghttpswwwnotionsoadaboost-8e7009e35aee4334b31d46bfd7e3dbba><a href=https://www.notion.so/AdaBoost-8e7009e35aee4334b31d46bfd7e3dbba>Adaptive boosting</a></a></li><li><a href=#gradient-boosting>Gradient boosting</a></li><li><a href=#extreme-gradient-boosting>Extreme gradient boosting</a></li></ol></li><li><a href=#reference>Reference</a><ol><li><a href=#xgboost>XGBoost</a></li><li><a href=#ensemble-learning>Ensemble Learning</a></li></ol></li></ol></nav></details></aside><p>XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the <strong>Gradient Boosting</strong> framework.</p><a href=#what-you-need-to-know-first><h1 id=what-you-need-to-know-first><span class=hanchor arialabel=Anchor># </span>What you need to know first</h1></a><ul><li><a href=/Deep-Learning-And-Machine-Learning/Deep_Learning_Block_and_Machine_Learning_Block/AdaBoost/ rel=noopener class=internal-link data-src=/Deep-Learning-And-Machine-Learning/Deep_Learning_Block_and_Machine_Learning_Block/AdaBoost/>🚧🚧AdaBoost</a></li></ul><a href=#what-is-xgboost><h1 id=what-is-xgboost><span class=hanchor arialabel=Anchor># </span>What is XGBoost</h1></a><p><strong>XGBoost</strong>, which stands for Extreme Gradient Boosting, is a scalable, distributed <strong>gradient-boosted</strong> decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.</p><p>It’s vital to an understanding of XGBoost to first grasp the machine learning concepts and algorithms that XGBoost builds upon: <strong>supervised machine learning</strong>, <strong>decision trees</strong>, <strong>ensemble learning</strong>, and <strong>gradient boosting</strong>.</p><p>Here, we need to know <strong>ensemble learning</strong> and <strong>gradient boosting,</strong> this two thing I don’t konw before.</p><a href=#what-is-ensemble-learning集成学习><h2 id=what-is-ensemble-learning集成学习><span class=hanchor arialabel=Anchor># </span>What is Ensemble Learning(集成学习)</h2></a><p><strong>Ensemble learning</strong> is a general meta approach to machine learning that <strong>seeks better predictive performance by combining the predictions from multiple models</strong>.</p><p>The three main classes of ensemble learning methods are <strong>bagging</strong>, <strong>stacking</strong>, and <strong>boosting.</strong></p><a href=#bagging><h3 id=bagging><span class=hanchor arialabel=Anchor># </span>Bagging</h3></a><p>Bagging means <strong>Bootstrap aggregation.</strong> It’s an ****ensemble learning method that seeks a diverse group of ensemble members by <strong>varying the training data</strong>.</p><p>This typically involves using a single machine learning algorithm, almost always an unpruned decision tree, and <strong>training each model on a different sample of the same training dataset.</strong> The predictions made by the ensemble members are then <strong>combined using simple statistics, such as voting or averaging.</strong></p><p>Key to the method is the manner in which each sample of the dataset is prepared to train ensemble members. Each model gets its own unique sample of the dataset.</p><p>Bagging adopts the <strong>bootstrap distribution</strong> for generating <strong>different base learners</strong>. In other words, it applies <strong>bootstrap sampling</strong> to obtain the data subsets for training the base learners.</p><p><img src=https://pinktalk.online//Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Untitled.png width=auto alt></p><aside>💡 **Bootstrap Sampling
Select a sample(a row of data), then reture the sample to dataset and re-select another sample to aggregate a data sample dataset. It means a sample can be selected zero, one, or mulitple times for a given dataset.**
Bootstrap sampling ****is often used in statistics with **small dataset**. geive a better overall estimate of the desired quantity than simply estimating from the whole dataset directly.</aside><p>Key word of bagging method:</p><ul><li><strong>Bootstrap Sampling</strong></li><li><strong>Voting or averaging of predictions</strong></li><li><strong>Unpruned decision tree</strong></li></ul><blockquote><p>Random forest is the typical example based on the bagging method.</p></blockquote><a href=#stacking><h3 id=stacking><span class=hanchor arialabel=Anchor># </span>Stacking</h3></a><p>Stacking means <strong>Stacked Generalization</strong>. It is an ensemble method that seeks a diverse group of members by <strong>varying the model types</strong> fit on the training data and using a model to combine predictions.</p><blockquote><p><em>Stacking is a general procedure where a learner is trained to combine the individual learners. Here, the individual learners are called the first-level learners, while the combiner is called the second-level learner, or meta-learner.</em></p></blockquote><p>Stacking has its own nomenclature where ensemble members are referred to as <strong>level-0 models</strong> and the model that is used to combine the predictions is referred to as a <strong>level-1 model</strong>.</p><p>The two-level hierarchy of models is the most common approach, although more layers of models can be used. For example, instead of a single level-1 model, we might have 3 or 5 level-1 models and a single level-2 model that combines the predictions of level-1 models in order to make a prediction.</p><p><img src=https://pinktalk.online//Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Untitled%201.png width=auto alt></p><p>Key words of stacknig method:</p><ul><li><strong>Unchanged training dataset</strong></li><li><strong>Different machine learning algorithms for each ensemble member</strong></li><li><strong>Machine learning model to learn how to best combine predictions</strong></li></ul><a href=#boosting><h3 id=boosting><span class=hanchor arialabel=Anchor># </span>Boosting</h3></a><p><strong>Boosting</strong> is an ensemble method that seeks to change the training data to focus attention on examples that previous fit models on the training dataset have gotten wrong.</p><blockquote><p><em>In boosting, […] the training dataset for each subsequent classifier increasingly focuses on instances misclassified by previously generated classifiers.</em></p></blockquote><p>The key property of boosting ensembles is the idea of <strong>correcting prediction errors</strong>. The models are fit and added to the ensemble sequentially such that the second model attempts to correct the predictions of the first model, the third corrects the second model, and so on.</p><p>This typically involves the use of very simple decision trees that only make a single or a few decisions, referred to in boosting as weak learners. The predictions of the weak learners are combined using simple voting or averaging, although <strong>the contributions are weighed proportional to their performance or capability</strong>. The objective is to develop a so-called “<em><strong>strong-learner</strong></em>” from many purpose-built “<em><strong>weak-learners</strong></em>”.</p><p>Typically, the training <strong>dataset is left unchanged</strong> and instead, the learning algorithm is modified to <strong>pay more or less attention to specific samples based on whether they have been predicted correctly or incorrectly</strong> by previously added ensemble members.</p><p><img src=https://pinktalk.online//Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Untitled%202.png width=auto alt></p><p>Key words to boosting method:</p><ul><li><strong>Bias training data</strong> toward those examples that are hard to predict</li><li><strong>Iteratively add ensemble members to correct predictions of prior models</strong></li><li>Combine predictions <strong>using a weighted average</strong> of models</li></ul><p><img src=https://pinktalk.online//Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Untitled%203.png width=auto alt></p><p>Type of boosting:</p><ul><li>Adaptive boosting</li><li>Gradient boosting</li><li>Extreme gradient boosting</li></ul><a href=#introduction-to-three-main-type-of-boosting-method><h1 id=introduction-to-three-main-type-of-boosting-method><span class=hanchor arialabel=Anchor># </span>Introduction to three main type of boosting method</h1></a><h2 id=adaptive-boostinghttpswwwnotionsoadaboost-8e7009e35aee4334b31d46bfd7e3dbba><a href=https://www.notion.so/AdaBoost-8e7009e35aee4334b31d46bfd7e3dbba rel=noopener>Adaptive boosting</a></h2><p>Adaptive Boosting (AdaBoost) was one of <strong>the earliest boosting models</strong> developed. It adapts and tries to <strong>self-correct</strong> in every iteration of the boosting process.</p><p>AdaBoost initially gives the same weight to each dataset. Then, it automatically adjusts the weights of the data points after every decision tree. It <strong>gives more weight to incorrectly classified items</strong> to correct them for the next round. It repeats the process until the residual error, or the difference between actual and predicted values, falls below an acceptable threshold.</p><p>You can use AdaBoost with many predictors, and it is typically not as sensitive as other boosting algorithms. This approach does not work well when there is a correlation among features or high data dimensionality. Overall, <strong>AdaBoost is a suitable type of boosting for classification problems</strong>.</p><p><strong>Must check Learning material below to know more detail of this algorithm. 🚧🚧🚧</strong></p><a href=#gradient-boosting><h2 id=gradient-boosting><span class=hanchor arialabel=Anchor># </span>Gradient boosting</h2></a><p>Gradient Boosting (GB) is similar to AdaBoost in that it, too, is a <strong>sequential training technique</strong>. The difference between AdaBoost and GB is that GB does not give incorrectly classified items more weight. Instead, GB software <strong>optimizes the loss function by generating base learners sequentially</strong> so that <strong>the present base learner is always more effective than the previous one</strong>. This method <strong>attempts to generate accurate results initially instead of correcting errors throughout the process</strong>, like AdaBoost. For this reason, GB software can lead to more accurate results. Gradient Boosting can help with both classification and regression-based problems.</p><p><img src=https://pinktalk.online//Deep%20Learning%20And%20Machine%20Learning/Deep_Learning_Block_and_Machine_Learning_Block/attachments/Untitled%204.png width=auto alt></p><a href=#extreme-gradient-boosting><h2 id=extreme-gradient-boosting><span class=hanchor arialabel=Anchor># </span>Extreme gradient boosting</h2></a><p>Extreme Gradient Boosting (XGBoost) improves gradient boosting for <strong>computational speed and scale</strong> in several ways. XGBoost uses multiple cores on the CPU so that learning can occur in parallel during training. It is a boosting algorithm that can handle extensive datasets, making it attractive for big data applications. The key features of XGBoost are parallelization, distributed computing, cache optimization, and out-of-core processing.</p><a href=#reference><h1 id=reference><span class=hanchor arialabel=Anchor># </span>Reference</h1></a><a href=#xgboost><h2 id=xgboost><span class=hanchor arialabel=Anchor># </span>XGBoost</h2></a><ul><li><p><a href=https://www.nvidia.com/en-us/glossary/data-science/xgboost/ rel=noopener>What is XGBoost?</a></p></li><li><p><a href="https://www.youtube.com/watch?v=OtD8wVaFm6E" rel=noopener>XGBoost Part 1 (of 4): Regression</a></p></li></ul><a href=#ensemble-learning><h2 id=ensemble-learning><span class=hanchor arialabel=Anchor># </span>Ensemble Learning</h2></a><ul><li><p><a href=https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/ rel=noopener>A Gentle Introduction to Ensemble Learning Algorithms - MachineLearningMastery.com</a></p></li><li><p><a href=https://blog.csdn.net/qq_36330643/article/details/77621232 rel=noopener>集成学习(ensemble learning)原理详解_Soyoger的博客-CSDN博客_ensemble l</a></p></li><li><p><a href=https://aws.amazon.com/what-is/boosting/ rel=noopener>What is Boosting? Guide to Boosting in Machine Learning - AWS</a></p></li><li><p><a href="https://www.youtube.com/watch?v=g9c66TUylZ4&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=45" rel=noopener>Regression Trees, Clearly Explained!!!</a></p></li><li><p><a href="https://www.youtube.com/watch?v=LsK-xG1cLYA" rel=noopener>AdaBoost, Clearly Explained</a></p></li><li><p><a href="https://www.youtube.com/watch?v=3CC4N4z3GJc" rel=noopener>Gradient Boost Part 1 (of 4): Regression Main Ideas</a></p></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Deep-Learning-And-Machine-Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Learning_Block_And_Machine_Learning_MOC/ data-ctx=XGBoost data-src=/Deep-Learning-And-Machine-Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Learning_Block_And_Machine_Learning_MOC class=internal-link>Deep Learning Block & Machine Learning - MOC</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pinktalk.online/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by JudeW using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://pinktalk.online/>Home</a></li><li><a href=https://twitter.com/PinkR1ver>Twitter</a></li><li><a href=https://github.com/PinkR1ver>GitHub</a></li></ul></footer></div></div></body></html>