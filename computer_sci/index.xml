<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer_scis on</title><link>https://pinktalk.online/computer_sci/</link><description>Recent content in Computer_scis on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://pinktalk.online/computer_sci/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://pinktalk.online/computer_sci/data_structure_and_algorithm/tree/fenwick_tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/data_structure_and_algorithm/tree/fenwick_tree/</guid><description>树状数组（Fenwick Tree），也被称为二叉索引树（Binary Indexed Tree，BIT），其初衷是解决数据压缩里的累积频率（Cumulative Frequency）的计算问题，现多用于高效计算数列的 前缀和， 区间和。它可以以$O(\log{n})$的时间得到任意前缀和，并同时支持在$O(\log{n})$时间内支持动态单点值的修改，空间复杂度为$O(n)$
我们希望BIT可以完成的操作是：
更改存储在索引I处的值。(这称为点更新操作) 查找长度为k的前缀之和。(这称为前缀和查询) Origin 按照Peter M.Fenwick的说法，正如所有的整数都可以表示成2的幂和，我们也可以把一串序列表示成一系列子序列的的和。采用这个想法，我们可将一个前缀和划分成多个子序列的和，而划分的方法与数的2的幂和具有极其相似的方式。一方面，子序列的个数是其二进制表示中1的个数，另一方面，子序列代表的f[i]的个数也是2的幂。
Step by Step lowbit(x:int) -&amp;gt; int 该函数返回参数转为二进制后,最后一个1的位置所代表的数值，例如：</description></item><item><title/><link>https://pinktalk.online/computer_sci/Hardware/MCU/Different-programming-interfaces/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Hardware/MCU/Different-programming-interfaces/</guid><description>What is programming interfaces in MCU A programming interface is a device that allows a programmer to connect to a microcontroller (MCU) and program it.</description></item><item><title>About coding language design detail</title><link>https://pinktalk.online/computer_sci/coding_knowledge/coding_lang_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/coding_knowledge/coding_lang_MOC/</guid><description>Python Why python doesn&amp;rsquo;t need pointer?
C MATLAB JavaScript</description></item><item><title>AdaBoost</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/AdaBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/AdaBoost/</guid><description>Video you need to watch first AdaBoost, Clearly Explained Key words and equation Stump(树桩) means classification just by one feature Amount of say $$ \text{Amout of say} = \frac{1}{2}\log{(\frac{1-\text{Total Error}}{\text{Total Error}})} $$</description></item><item><title>Breadth First Search in Python</title><link>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/BFS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/BFS/</guid><description>Basic Concept Code Implementation Reference Breadth First Search Algorithm Explained (With Example and Code). www.youtube.com, https://www.youtube.com/watch?v=YtD2KGRdn3s. Accessed 19 July 2023.</description></item><item><title>Computational Geometry MOC</title><link>https://pinktalk.online/computer_sci/computational_geometry/MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/computational_geometry/MOC/</guid><description>3D Geometry Algorithm Delaunay Triangulation</description></item><item><title>Data Structure and Algorithm MOC</title><link>https://pinktalk.online/computer_sci/data_structure_and_algorithm/MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/data_structure_and_algorithm/MOC/</guid><description>Tree-like Structure Fenwick Tree Segment Tree Graph Algorithm BFS Topological Sorting Minimum Spanning Tree Type of graph Spanning Tree</description></item><item><title>Decision Tree</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Decision_Tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Decision_Tree/</guid><description>Only vedio here:
Decision and Classification Trees, Clearly Explained!!! Regression Trees, Clearly Explained!!!</description></item><item><title>Deep Learning - MOC</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Deep-_Learning_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Deep-_Learning_MOC/</guid><description>Tech Explanation ⭐Deep Learning MOC
✨Machine Learning MOC
LLM - MOC</description></item><item><title>Deep Learning MOC</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/deep_learning_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/deep_learning_MOC/</guid><description>Attention is all you need [[computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/⭐Attention|Attention Blocker]] [[computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Transformer|Transformer]] Tree-like architecture Decision Tree Random Forest Deep Neural Decision Forests XGBoost Ensemble Learning AdaBoost XGBoost Time-series dealing block LSTM Clustering Algorithm K-means Clustering Algorithm</description></item><item><title>Deep Neural Decision Forests</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Deep_Neural_Decision_Forests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Deep_Neural_Decision_Forests/</guid><description>Background Decision Tree Random Forest What is Deep Neural Decision Forests Deep Neural Decision Forests(dNDFs)是Neural Networks和Random Forest的结合，但是它更倾向于Neural Networks。它本质上是Nerual Networks incorporate Random Forest来提高NN的效率和准确度，训练方法和NN一致。</description></item><item><title>DeepAR - Time Series Forcasting</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR/</guid><description>DeepAR, an autoregressive recurrent network developed by Amazon, is the first model that could natively work on multiple time-series. It&amp;rsquo;s a milestone in time-series community.</description></item><item><title>Delaunay Triangulation</title><link>https://pinktalk.online/computer_sci/computational_geometry/delaunay_triangulation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/computational_geometry/delaunay_triangulation/</guid><description>What is Delaunay Triangulation? Reference Delaunay Triangulation (1/5) | Computational Geometry - Lecture 08. www.youtube.com, https://www.youtube.com/watch?v=6UsdvbiJx54. Accessed 4 Sept.</description></item><item><title>Dynamic Time Warping (DTW)</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Trick/DTW/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Trick/DTW/</guid><description>欧氏距离在时间序列之间可能是一个不好的选择，因为时间轴上存在扭曲的情况。DTW 是一个考虑到这种扭曲的，测量距离来比较两个时间序列的一个指标，本section讲解如何计算 DTW distance
Detail Step 1. 准备输入序列 假设两个time series, A &amp;amp; B
Step 2. 计算距离矩阵 创建一个距离矩阵，其中的元素表示序列 A 和序列 B 中每个时间点之间的距离。常见的距离度量方法包括欧氏距离、曼哈顿距离、余弦相似度等。根据你的数据类型和需求选择适当的距离度量方法。</description></item><item><title>Famous Model MOC</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC/</guid><description>Time-series DeepAR</description></item><item><title>Gated Recurrent Unit</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/GRU/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/GRU/</guid><description/></item><item><title>Hardware - MOC</title><link>https://pinktalk.online/computer_sci/Hardware/Hardware_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Hardware/Hardware_MOC/</guid><description>Microcontroller unit (MCU) Basic concepts Different programming interfaces</description></item><item><title>How to do use fine tune tech to create your chatbot</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune/</guid><description/></item><item><title>How to make custom dataset?</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/dataset/make_custom_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/dataset/make_custom_dataset/</guid><description/></item><item><title>K-means Clustering Algorithm</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/clustering/k-means/k_means/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/clustering/k-means/k_means/</guid><description>Step by Step Our algorithm works as follows, assuming we have inputs $x_1, x_2, \cdots, x_n$ and value of $K$</description></item><item><title>LangChain Explained</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/langchain/langchain_basic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/langchain/langchain_basic/</guid><description>What is LangChain LangChain is an open source framework that allows AI developers to combine LLMs like GPT-4 with external sources of computation and data.</description></item><item><title>Large Language Model(LLM) - MOC</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</guid><description>Training Training Tech Outline ⭐⭐⭐Train LLM from scratch ⭐⭐⭐Detailed explanation of RLHF technology How to do use fine tune tech to create your chatbot Learn finetune by Stanford Alpaca Metrics How to evaluate a LLM performance?</description></item><item><title>Learn finetune by Stanford Alpaca</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca/</guid><description> Reference https://www.youtube.com/watch?v=pcszoCYw3vc https://crfm.stanford.edu/2023/03/13/alpaca.html</description></item><item><title>LLM hyperparameter</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter/</guid><description>LLM Temperature Temperature definition come from the physical meaning of temperature. The more higher temperature, the atoms moving more faster, meaning more randomness.</description></item><item><title>LLM training steps</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/steps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/steps/</guid><description>训练大型语言模型（LLM）的方法通常涉及以下步骤：
数据收集：收集大规模的文本数据作为训练数据。这些数据可以是互联网上的文本、书籍、文章、新闻、对话记录等。数据的质量和多样性对于训练出高质量的LLM非常重要。
预处理：对数据进行预处理以使其适合模型训练。这包括分词（将文本划分为词或子词单元）、建立词汇表（将词映射到数字表示）、清理和规范化文本等操作。
构建模型架构：选择适当的模型架构来构建LLM。目前最常用的模型架构是Transformer，其中包含多层的自注意力机制和前馈神经网络层。
预训练：使用大规模的文本数据集对模型进行预训练。预训练是指在无监督的情况下，通过让模型学习预测缺失的词语或下一个词语等任务来提取语言知识。这使得模型能够学习到丰富的语言表示。
微调（Fine-tuning）：在预训练之后，使用特定的任务数据对模型进行微调。微调是指在特定任务的标注数据上进行有监督的训练，例如文本生成、问题回答等。通过微调，模型可以更好地适应特定任务的要求。
超参数调优：调整模型的超参数，例如学习率、批量大小、模型层数等，以获得更好的性能和效果。
评估和迭代：对训练后的模型进行评估，并根据评估结果进行迭代改进。这可能包括调整模型架构、增加训练数据、调整训练策略等。</description></item><item><title>Long Short-Term Memory Networks</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/LSTM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/LSTM/</guid><description>[!quote] When I was learning LSTM, the new deep learning block Transformers dominate the NLP field. However, Transformers don&amp;rsquo;t decisively outperform LSTMS in time-series-related tasks.</description></item><item><title>Machine Learning MOC</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/MOC/</guid><description> SVM</description></item><item><title>Matplotlib Backend Review</title><link>https://pinktalk.online/computer_sci/coding_knowledge/python/matplotlib_backend/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/coding_knowledge/python/matplotlib_backend/</guid><description/></item><item><title>Minimum Spanning Tree</title><link>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/MST/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/MST/</guid><description>Not now&amp;hellip;</description></item><item><title>Model Evaluation - MOC</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC/</guid><description> Model Evaluation in Time Series Forecasting</description></item><item><title>Model Evaluation in Time Series Forecasting</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting/</guid><description>Some famous time series scoring technics MAE, RMSE and AIC Mean Forecast Accuracy Warning: The time series model EVALUATION TRAP!</description></item><item><title>Model Interpretability - MOC</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC/</guid><description> SHAP</description></item><item><title>Quantile loss</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Trick/quantile_loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Trick/quantile_loss/</guid><description>在大多数现实世界的预测问题中，我们的预测所带来的不确定性具有重要价值。相较于仅仅提供点估计，了解预测范围能够显著改善许多商业应用的决策过程。Quantile loss就是为例帮助我们了解预测范围的loss function。
Quantile loss用于衡量预测分布和目标分布之间的差异，特别适用于处理不确定性较高的预测问题。
What is quantile Quantile
What is a prediction interval 预测区间是对预测的不确定性进行量化的一种方法。它为结果变量的估计提供了概率上限和下限的范围。
输出本身是随机变量，因此具有分布特性。预测区间的目的在于了解结果的正确性可能性。
What is Quantile Loss 在Quantile loss中，我们将预测结果和目标值都表示为分位数形式，例如，我们可以用预测的α分位数来表示预测结果，用真实值的α分位数来表示目标值。然后，Quantile loss衡量了这两个分布之间的差异，通常使用分位数损失函数来计算。</description></item><item><title>Random Forest</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Random_Forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Random_Forest/</guid><description>Background Decision Tree Detail only vedio here:
StatQuest: Random Forests Part 1 - Building, Using and Evaluating</description></item><item><title>Reinforcement Learning from Human Feedback</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/RLHF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/RLHF/</guid><description>Review: Reinforcement Learning Basics Reinforcement learning is a mathematical framework.
Demystify the reinforcement learning model, it&amp;rsquo;s a open-ended model using reward function to optimize agent to solve complex task in target environment.</description></item><item><title>Segment Tree</title><link>https://pinktalk.online/computer_sci/data_structure_and_algorithm/tree/segment_tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/data_structure_and_algorithm/tree/segment_tree/</guid><description>Overview Segment Tree（线段树）是一种用于解决区间查询问题的数据结构。它可以有效地处理包含大量区间操作的问题，如查询区间最大值、最小值、求和、更新等。
Segment Tree将给定的区间划分为若干个较小的子区间，并使用树进行表示。每个节点表示一个子区间，树的根节点表示整个区间。每个节点记录了对应子区间的一些统计信息，如该区间的最大值、最小值、总和等。
构建Segment Tree的过程中，首先将问题规模不断缩小，将大的区间划分为两个较小的子区间，并依次递归构建每个子区间的节点。当区间缩小到长度为1时，即叶子节点，将问题的原始数据作为叶子节点的值。
Segment Tree的构建完成后，可以高效地进行查询和更新操作。查询操作通过递归遍历树的节点，在给定的区间范围内查找所需的统计信息。更新操作通过递归更新树的节点，更新目标区间内的值，并更新父节点的统计信息。
由于Segment Tree的每个节点代表的区间是互不重叠的，因此在进行统计信息的查询和更新时，可以利用区间的性质进行剪枝操作，从而提高效率。
Detail Basic Segment Tree is a basically binary tree, we can represent segment tree in a simple linear array.</description></item><item><title>SHAP - a reliable way to analyze model interpretability</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP/</guid><description>SHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations
Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations</description></item><item><title>Spanning Tree</title><link>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/spanning_tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/spanning_tree/</guid><description>What is Spanning Tree? 树上再加一条边使之存在环，就称为基环树
Example:
Why do we need Spanning Tree Network design: Spanning trees are used to create efficient and redundant networks, such as in Ethernet networks or telecommunications.</description></item><item><title>Support Vector Machine</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/SVM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/machine_learning/SVM/</guid><description>Overview Hyper Parameters Kernel Function Linear Polynomial RBF $\gamma$: The gamma parameter defines the influence of each training example on the decision boundary.</description></item><item><title>Tasks to evaluate BERT - Maybe can be deployed in other LM</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/</guid><description>Overview MNLI-m (Multi-Genre Natural Language Inference - Matched): MNLI-m is a benchmark dataset and task for natural language inference (NLI).</description></item><item><title>Temporal Fusion Transformer</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/Temporal_Fusion_Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Famous_Model/Temporal_Fusion_Transformer/</guid><description/></item><item><title>Tokenization</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization/</guid><description/></item><item><title>Topological Sorting</title><link>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/topological_sorting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/data_structure_and_algorithm/graph/topological_sorting/</guid><description>What is Topological Sorting Topological Sorting(拓扑排序) is designed for Directed Acyclic Graph(DAG, 有向无环图). Topological Sorting is a linear ordering of vertices such that for every directed edge u v, vertex u comes before v in the ordering.</description></item><item><title>Train LLM from scratch</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM/</guid><description>Find a dataset Find a corpus of text in language you prefer.
Such as OSCAR Intuitively, the more data you can get to pretrain on, the better results you will get.</description></item><item><title>Transformer</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Transformer/</guid><description> [!info] 在学习Transformer前，你需要学习 ⭐Attention
Transformer 是Seq2Seq model，由Encoder和Decoder组成 Encoder 这里贴的是原文Encoder的架构</description></item><item><title>Why python doesn't need pointer?</title><link>https://pinktalk.online/computer_sci/coding_knowledge/python/python_doesnt_need_pointer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/coding_knowledge/python/python_doesnt_need_pointer/</guid><description>Python doesn&amp;rsquo;t require the explicit use of pointers like C because of its underlying memory management and object model.
Design Concept Underlying memory management In Python, variables are references to objects rather than memory addresses like pointers in C.</description></item><item><title>XGBoost</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/XGBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/XGBoost/</guid><description>XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the Gradient Boosting framework.</description></item><item><title>⭐Attenion</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/Attention/</guid><description>Self-Attention 讲述self-attention我们以sequence labeling任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。
典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。
我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助self-attention层。
Detail 对于Self-attention层，生成的$b^i$向量是考虑到所有输入$\sum_i\alpha^i$向量
Vector Relevance Step 1. 使用Dot-product 去计算 vector relevance Step 2.</description></item></channel></rss>