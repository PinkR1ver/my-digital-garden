<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="SHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations
Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations"><meta property="og:title" content="SHAP - a reliable way to analyze model interpretability"><meta property="og:description" content="SHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations
Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations"><meta property="og:type" content="website"><meta property="og:image" content="https://pinktalk.online/icon.png"><meta property="og:url" content="https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="SHAP - a reliable way to analyze model interpretability"><meta name=twitter:description content="SHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations
Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations"><meta name=twitter:image content="https://pinktalk.online/icon.png"><meta name=twitter:site content="PinkR1ver"><title>SHAP - a reliable way to analyze model interpretability</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://pinktalk.online//icon.png><link href=https://pinktalk.online/styles.f342c0370dcc71a4a5d1fac5b401c180.min.css rel=stylesheet><link href=https://pinktalk.online/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://pinktalk.online/js/darkmode.4b1d856da524d844cfa5256b4795119b.min.js></script>
<script src=https://pinktalk.online/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://pinktalk.online/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://pinktalk.online/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://pinktalk.online/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://pinktalk.online/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://pinktalk.online/",fetchData=Promise.all([fetch("https://pinktalk.online/indices/linkIndex.96092c7ac55aee1476ae4872fcbc88d0.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://pinktalk.online/indices/contentIndex.2f9196f17f9290e4351b78fe7171b6e5.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://pinktalk.online",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://pinktalk.online",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:2,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/pinktalk.online\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=pinktalk.online src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://pinktalk.online/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://pinktalk.online/>🎣JudeW's Digital Garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>SHAP - a reliable way to analyze model interpretability</h1><p class=meta>Last updated
Sep 4, 2023
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://pinktalk.online/tags/deep-learning/>Deep learning</a></li><li><a href=https://pinktalk.online/tags/interpretability/>Interpretability</a></li><li><a href=https://pinktalk.online/tags/algorithm/>Algorithm</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#mathematical-and-algorithm-foundation>Mathematical and Algorithm Foundation</a><ol><li><a href=#shapely-values>Shapely Values</a></li><li><a href=#shapely-additive-explanations>Shapely Additive Explanations</a><ol><li><a href=#local-accuracy>Local accuracy</a></li><li><a href=#missingness>Missingness</a></li><li><a href=#consistency>Consistency</a></li></ol></li></ol></li><li><a href=#why-shap>Why SHAP</a></li><li><a href=#shap-step-by-step-process-same-as-shapexplainer>SHAP, step-by-step Process, same as shap.explainer</a></li><li><a href=#shapley-kernel>Shapley kernel</a><ol><li><a href=#too-many-coalitions-need-to-be-sampled>Too many coalitions need to be sampled</a></li><li><a href=#detail>Detail</a><ol><li><a href=#different-types-of-shap>Different types of SHAP</a></li><li><a href=#you-need-to-notice>You need to notice</a></li></ol></li></ol></li><li><a href=#reference>Reference</a></li><li><a href=#appendix>Appendix</a></li></ol></nav></details></aside><p>SHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for <strong>SH</strong>apley <strong>A</strong>dditive ex<strong>P</strong>lanations</p><p>Shapely values are obtained by incorporating concepts from <em>Cooperative Game Theory</em>  and <em>local explanations</em></p><a href=#mathematical-and-algorithm-foundation><h1 id=mathematical-and-algorithm-foundation><span class=hanchor arialabel=Anchor># </span>Mathematical and Algorithm Foundation</h1></a><a href=#shapely-values><h2 id=shapely-values><span class=hanchor arialabel=Anchor># </span>Shapely Values</h2></a><p>Shapely values were from game theory and invented by Lloyd Shapley. Shapely values were invented to be a way of providing a fair solution to the following question:</p><blockquote class=question-callout><p>If we have a coalition <strong>C</strong> that collaborates to produce a value <strong>V</strong>: How much did each individual member contribute to the final value</p></blockquote><p>The method here we assess each individual member’s contribution is to removing each member to get a new coalition and then compare their production, like this graphs:</p><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165429.png width=auto alt></p><p>And then, we get every member 1 included or not included coalitions like this:</p><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165523.png width=auto alt></p><p>Using left value - right value, we can get difference like image left above; And then we calculate the mean of them:</p><p>$$
\varphi_i=\frac{1}{\text{Members}}\sum_{\forall \text{C s.t. i}\notin \text{C}} \frac{\text{Marginal Contribution of i to C}}{\text{Coalitions of size |C|}}
$$</p><a href=#shapely-additive-explanations><h2 id=shapely-additive-explanations><span class=hanchor arialabel=Anchor># </span>Shapely Additive Explanations</h2></a><p>We need to know what’s <strong>additive</strong> mean here. Lundberg and Lee define an additive feature attribution as follows:</p><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165623.png width=auto alt></p><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165818.png width=auto alt></p><p>$x&rsquo;$, the simplified local inputs usually means that we turn a feature vector into a discrete binary vector, where features are either included or excluded. Also, the $g(x&rsquo;)$ should take this form:</p><p>$$
g(x&rsquo;)=\varphi_0+\sum_{i=1}^N \varphi_i {x&rsquo;}_i
$$</p><ul><li>$\varphi_0$ is the <strong>null output</strong> of this model, that is, the <strong>average output</strong> of this model</li></ul><ul><li>$\varphi_i$ is <strong>feature affect</strong>, is how much that feature changes the output of the model, introduced above. It’s called <strong>attribution</strong></li></ul><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329165840.png width=auto alt></p><p>Now Lundberg and Lee go on to describe a set of three desirable properties of such an additive feature method, <strong>local accuracy</strong>, <strong>missingness</strong>, and <strong>consistency</strong>.</p><a href=#local-accuracy><h3 id=local-accuracy><span class=hanchor arialabel=Anchor># </span>Local accuracy</h3></a><p>$$
g(x&rsquo;)\approx f(x) \quad \text{if} \quad x&rsquo;\approx x
$$</p><a href=#missingness><h3 id=missingness><span class=hanchor arialabel=Anchor># </span>Missingness</h3></a><p>$$
{x_i}&rsquo; = 0 \rightarrow \varphi_i = 0
$$</p><p>if a feature excluded from the model. it’s attribution must be zero; that is, the only thing that can affect the output of the explanation model is the inclusion of features, not the exclusion.</p><a href=#consistency><h3 id=consistency><span class=hanchor arialabel=Anchor># </span>Consistency</h3></a><p>If feature contribution changes, the feature effect cannot change in the opposite direction</p><a href=#why-shap><h1 id=why-shap><span class=hanchor arialabel=Anchor># </span>Why SHAP</h1></a><p>Lee and Lundberg in their paper argue that only SHAP satisfies all three properties if <strong>the feature attributions in only additive explanatory model are specifically chosen to be the shapley values of those features</strong></p><a href=#shap-step-by-step-process-same-as-shapexplainer><h1 id=shap-step-by-step-process-same-as-shapexplainer><span class=hanchor arialabel=Anchor># </span>SHAP, step-by-step Process, same as shap.explainer</h1></a><p>For example, we consider a ice cream shop in the airport, it has four features we can know to predict his business.</p><p>$$
\begin{bmatrix}
\text{temperature} & \text{day of weeks} & \text{num of flights} & \text{num of hours}
\end{bmatrix}
\\ \rightarrow \\ \begin{bmatrix}
T & D & F & H
\end{bmatrix}
$$</p><p>For, example, we want to know the temperature 80 in sample [80 1 100 4] shapley value, here’s the step</p><ul><li>Step 1. Get random permutation of features, and give a bracket to the feature we care and everything in its right. (manually)</li></ul><p>$$
\begin{bmatrix}
F & D & \underbrace{T \quad H}
\end{bmatrix}
$$</p><ul><li>Step 2. Pick random sample from dataset</li></ul><p>For example, [200 5 70 8], form: [F D T H]</p><ul><li>Step 3. Form vectors $x_1 \quad x_2$</li></ul><p>$$
x_1=[100 \quad 1 \quad 80 \quad \color{#BF40BF} 8 \color{#FFFFFF}]
$$</p><p>$x_1$ is partially from original sample and partially from the random chosen one, the feature in bracket will from random chosen one, exclude what we care</p><p>$$
x_2 = [100 \quad 1 \quad \color{#BF40BF} 70 \quad 8 \color{#FFFFFF}]
$$</p><p>$x_2$ just change the feature we care into the same as random chosen one’s feature value</p><p>Then, calculate the diff and record</p><p>$$
DIFF = c_1 - c_2
$$</p><ul><li>Step 4. Record the diff & return to step 1. and repeat many times</li></ul><p>$$
\text{SHAP}(T=80 | [80 \quad 1 \quad 100 \quad 4]) = \text{average(DIFF)}
$$</p><a href=#shapley-kernel><h1 id=shapley-kernel><span class=hanchor arialabel=Anchor># </span>Shapley kernel</h1></a><a href=#too-many-coalitions-need-to-be-sampled><h2 id=too-many-coalitions-need-to-be-sampled><span class=hanchor arialabel=Anchor># </span>Too many coalitions need to be sampled</h2></a><p>Like we introduce shapley values above, for each $\varphi_i$ we need to sample a lot of coalitions to compute the difference.</p><p>For 4 features, we need 64 total coalitions to sample; For 32 features, we need 17.1 billion coalitions to sample.</p><p>It’s entirely untenable.</p><p>So, to get over this difficulty, we need devise a <strong>shapley kernel</strong>, and that’s how the Lee and Lundberg do</p><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329181956.png width=auto alt></p><a href=#detail><h2 id=detail><span class=hanchor arialabel=Anchor># </span>Detail</h2></a><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329182011.png width=auto alt></p><p>Though most of ML models won’t just let you omit a feature, what we do is define a <strong>background dataset</strong> B, one that contains a set of representative data points that model was trained over. We then filled in out omitted feature of features with values from background dataset, while holding the features are included in the permutation fixed to their original values. We then take the average of the model output over all of these new synthetic data point as our model output for that feature permutation which we call $\bar{y}$.</p><p>$$
E[y_{\text{12i4}}\\ \\ \forall \\ \text{i}\in B] = \bar{y}_{\text{124}}
$$
<img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329205039.png width=auto alt></p><p>Them we have a number of samples computed in this way,like image in left.</p><p>We can formulate this as a weighted linear regression, with each feature assigned a coefficient.</p><p>And we can prove that, in the special choice, the coefficient can be the shaplely values. <strong>This weighting scheme is the basis of the Shapley Kernal.</strong> In this situation, the weighted linear regression process as a whole is Kernal SHAP.</p><a href=#different-types-of-shap><h3 id=different-types-of-shap><span class=hanchor arialabel=Anchor># </span>Different types of SHAP</h3></a><ul><li><strong>Kernal SHAP</strong></li><li>Low-order SHAP</li><li>Linear SHAP</li><li>Max SHAP</li><li>Deep SHAP</li><li>Tree SHAP</li></ul><p><img src=https://pinktalk.online//computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/attachments/Pasted%20image%2020230329205130.png width=auto alt></p><a href=#you-need-to-notice><h3 id=you-need-to-notice><span class=hanchor arialabel=Anchor># </span>You need to notice</h3></a><p>We can see that, we calculate shapley values using linear regression lastly. So there must be the error here, but some python packages can not give us the error bound, so it’s confusion to konw if this error come from linear regression or the data, or the model.</p><a href=#reference><h1 id=reference><span class=hanchor arialabel=Anchor># </span>Reference</h1></a><p><a href="https://www.youtube.com/watch?v=VB9uV-x0gtg" rel=noopener>Shapley Additive Explanations (SHAP)</a></p><p><a href=https://towardsdatascience.com/shap-a-reliable-way-to-analyze-your-model-interpretability-874294d30af6 rel=noopener>SHAP: A reliable way to analyze your model interpretability</a></p><p><a href=https://zhuanlan.zhihu.com/p/483622352 rel=noopener>【Python可解释机器学习库SHAP】：Python的可解释机器学习库SHAP</a></p><p><a href="https://www.youtube.com/watch?v=NBg7YirBTN8" rel=noopener>Shapley Values : Data Science Concepts</a></p><a href=#appendix><h1 id=appendix><span class=hanchor arialabel=Anchor># </span>Appendix</h1></a><p>Other methods to interprete model:</p><p><a href=https://paperswithcode.com/method/shap rel=noopener>Papers with Code - SHAP Explained</a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC/ data-ctx=SHAP data-src=/computer_sci/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC class=internal-link>Model Interpretability - MOC</a></li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://pinktalk.online/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by JudeW using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://pinktalk.online/>Home</a></li><li><a href=https://twitter.com/PinkR1ver>Twitter</a></li><li><a href=https://github.com/PinkR1ver>GitHub</a></li><li><a href=https://www.instagram.com/jude.wang.yc/>Instagram</a></li></ul></footer></div></div></body></html>