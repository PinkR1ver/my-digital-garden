<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep_Learning_And_Machine_Learnings on</title><link>https://pinktalk.online/deep_learning_and_machine_learning/</link><description>Recent content in Deep_Learning_And_Machine_Learnings on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://pinktalk.online/deep_learning_and_machine_learning/index.xml" rel="self" type="application/rss+xml"/><item><title>AdaBoost</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/AdaBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/AdaBoost/</guid><description>Video you need to watch first AdaBoost, Clearly Explained Key words and equation Stump(树桩) means classification just by one feature Amount of say $$ \text{Amout of say} = \frac{1}{2}\log{(\frac{1-\text{Total Error}}{\text{Total Error}})} $$</description></item><item><title>Decision Tree</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Decision_Tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Decision_Tree/</guid><description>Only vedio here:
Decision and Classification Trees, Clearly Explained!!! Regression Trees, Clearly Explained!!!</description></item><item><title>Deep Learning - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep-_Learning_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep-_Learning_MOC/</guid><description>Deep Learning Block &amp;amp; Machine Learning - MOC
Model Interpretability
Famous Model - MOC</description></item><item><title>Deep Learning Block &amp; Machine Learning - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Learning_Block_And_Machine_Learning_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Learning_Block_And_Machine_Learning_MOC/</guid><description>Attention is all you need [[Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/⭐Attention|Attention Blocker]] [[Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Transformer|Transformer]] Tree-like architecture Decision Tree Random Forest Deep Neural Decision Forests XGBoost Ensemble Learning AdaBoost XGBoost Time-series dealing block LSTM</description></item><item><title>Deep Neural Decision Forests</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Neural_Decision_Forests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Neural_Decision_Forests/</guid><description>Background Decision Tree Random Forest What is Deep Neural Decision Forests Deep Neural Decision Forests(dNDFs)是Neural Networks和Random Forest的结合，但是它更倾向于Neural Networks。它本质上是Nerual Networks incorporate Random Forest来提高NN的效率和准确度，训练方法和NN一致。</description></item><item><title>DeepAR - Time Series Forcasting</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR/</guid><description>DeepAR, an autoregressive recurrent network developed by Amazon, is the first model that could natively work on multiple time-series. It&amp;rsquo;s a milestone in time-series community.</description></item><item><title>Dynamic Time Warping (DTW)</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Trick/DTW/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Trick/DTW/</guid><description>欧氏距离在时间序列之间可能是一个不好的选择，因为时间轴上存在扭曲的情况。DTW 是一个考虑到这种扭曲的，测量距离来比较两个时间序列的一个指标，本section讲解如何计算 DTW distance
Detail Step 1. 准备输入序列 假设两个time series, A &amp;amp; B
Step 2. 计算距离矩阵 创建一个距离矩阵，其中的元素表示序列 A 和序列 B 中每个时间点之间的距离。常见的距离度量方法包括欧氏距离、曼哈顿距离、余弦相似度等。根据你的数据类型和需求选择适当的距离度量方法。</description></item><item><title>Famous Model MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC/</guid><description>Time-series DeepAR</description></item><item><title>Gated Recurrent Unit</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/GRU/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/GRU/</guid><description/></item><item><title>How to do use fine tune tech to create your chatbot</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/how_to_fine_tune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/how_to_fine_tune/</guid><description/></item><item><title>Large Language Model(LLM) - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</guid><description>Training How to do use fine tune tech to create your chatbot
Metrics How to evaluate a LLM performance?</description></item><item><title>Long Short-Term Memory Networks</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/LSTM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/LSTM/</guid><description>[!quote] When I was learning LSTM, the new deep learning block Transformers dominate the NLP field. However, Transformers don&amp;rsquo;t decisively outperform LSTMS in time-series-related tasks.</description></item><item><title>Model Evaluation - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC/</guid><description> Model Evaluation in Time Series Forecasting</description></item><item><title>Model Evaluation in Time Series Forecasting</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting/</guid><description>Some famous time series scoring technics MAE, RMSE and AIC Mean Forecast Accuracy Warning: The time series model EVALUATION TRAP!</description></item><item><title>Model Interpretability - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC/</guid><description> SHAP</description></item><item><title>Quantile loss</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Trick/quantile_loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Trick/quantile_loss/</guid><description>在大多数现实世界的预测问题中，我们的预测所带来的不确定性具有重要价值。相较于仅仅提供点估计，了解预测范围能够显著改善许多商业应用的决策过程。Quantile loss就是为例帮助我们了解预测范围的loss function。
Quantile loss用于衡量预测分布和目标分布之间的差异，特别适用于处理不确定性较高的预测问题。
What is quantile Quantile
What is a prediction interval 预测区间是对预测的不确定性进行量化的一种方法。它为结果变量的估计提供了概率上限和下限的范围。
输出本身是随机变量，因此具有分布特性。预测区间的目的在于了解结果的正确性可能性。
What is Quantile Loss 在Quantile loss中，我们将预测结果和目标值都表示为分位数形式，例如，我们可以用预测的α分位数来表示预测结果，用真实值的α分位数来表示目标值。然后，Quantile loss衡量了这两个分布之间的差异，通常使用分位数损失函数来计算。</description></item><item><title>Random Forest</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Random_Forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Random_Forest/</guid><description>Background Decision Tree Detail only vedio here:
StatQuest: Random Forests Part 1 - Building, Using and Evaluating</description></item><item><title>SHAP - a reliable way to analyze model interpretability</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP/</guid><description>SHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations
Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations</description></item><item><title>Temporal Fusion Transformer</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Temporal_Fusion_Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Temporal_Fusion_Transformer/</guid><description/></item><item><title>Transformer</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Transformer/</guid><description> [!info] 在学习Transformer前，你需要学习 ⭐Attention
Transformer 是Seq2Seq model，由Encoder和Decoder组成 Encoder 这里贴的是原文Encoder的架构</description></item><item><title>XGBoost</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/XGBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/XGBoost/</guid><description>XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the Gradient Boosting framework.</description></item><item><title>⭐Attenion</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Attention/</guid><description>Self-Attention 讲述self-attention我们以sequence labeling任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。
典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。
我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助self-attention层。
Detail 对于Self-attention层，生成的$b^i$向量是考虑到所有输入$\sum_i\alpha^i$向量
Vector Relevance Step 1. 使用Dot-product 去计算 vector relevance Step 2.</description></item></channel></rss>