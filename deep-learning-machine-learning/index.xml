<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning &amp; Machine Learnings on</title><link>https://pinkr1ver.github.io/quartz/deep-learning-machine-learning/</link><description>Recent content in Deep Learning &amp; Machine Learnings on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://pinkr1ver.github.io/quartz/deep-learning-machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://pinkr1ver.github.io/quartz/Deep-Learning-Machine-Learning/Deep-Learning-Block/Deep-Learning-Block-Table-of-Content/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinkr1ver.github.io/quartz/Deep-Learning-Machine-Learning/Deep-Learning-Block/Deep-Learning-Block-Table-of-Content/</guid><description>[[Deep Learning &amp;amp; Machine Learning/Deep Learning Block/⭐Attention | Attention Blocker]] [[Deep Learning &amp;amp; Machine Learning/Deep Learning Block/Transformer]]</description></item><item><title/><link>https://pinkr1ver.github.io/quartz/Deep-Learning-Machine-Learning/Deep-Learning-Block/Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinkr1ver.github.io/quartz/Deep-Learning-Machine-Learning/Deep-Learning-Block/Transformer/</guid><description>[!info] 在学习Transformer前，你需要学习 [[Deep Learning &amp;amp; Machine Learning/Deep Learning Block/⭐Attention]]
Transformer 是Seq2Seq model，由Encoder和Decoder组成 ![[Deep Learning &amp;amp; Machine Learning/Deep Learning Block/attachments/Pasted image 20230316160103.</description></item><item><title/><link>https://pinkr1ver.github.io/quartz/Deep-Learning-Machine-Learning/Deep-Learning-Block/Attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinkr1ver.github.io/quartz/Deep-Learning-Machine-Learning/Deep-Learning-Block/Attention/</guid><description>Self-Attention 讲述self-attention我们以sequence labeling任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。
典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。
![[Deep Learning &amp;amp; Machine Learning/Deep Learning Block/attachments/Pasted image 20230315195403.png]]
我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助self-attention层。
Detail ![[Deep Learning &amp;amp; Machine Learning/Deep Learning Block/attachments/Pasted image 20230315195603.</description></item></channel></rss>