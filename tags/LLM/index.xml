<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on</title><link>https://pinktalk.online/tags/LLM/</link><description>Recent content in LLM on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://pinktalk.online/tags/LLM/index.xml" rel="self" type="application/rss+xml"/><item><title>How to do use fine tune tech to create your chatbot</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune/</guid><description/></item><item><title>How to make custom dataset?</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/dataset/make_custom_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/dataset/make_custom_dataset/</guid><description/></item><item><title>LangChain Explained</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/langchain/langchain_basic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/langchain/langchain_basic/</guid><description>What is LangChain LangChain is an open source framework that allows AI developers to combine LLMs like GPT-4 with external sources of computation and data.</description></item><item><title>Large Language Model(LLM) - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</guid><description>Training Training Tech Outline ⭐⭐⭐Train LLM from scratch ⭐⭐⭐Detailed explanation of RLHF technology How to do use fine tune tech to create your chatbot Learn finetune by Stanford Alpaca Metrics How to evaluate a LLM performance?</description></item><item><title>Learn finetune by Stanford Alpaca</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca/</guid><description> Reference https://www.youtube.com/watch?v=pcszoCYw3vc https://crfm.stanford.edu/2023/03/13/alpaca.html</description></item><item><title>LLM hyperparameter</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter/</guid><description>LLM Temperature Temperature definition come from the physical meaning of temperature. The more higher temperature, the atoms moving more faster, meaning more randomness.</description></item><item><title>LLM training steps</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/steps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/steps/</guid><description>训练大型语言模型（LLM）的方法通常涉及以下步骤：
数据收集：收集大规模的文本数据作为训练数据。这些数据可以是互联网上的文本、书籍、文章、新闻、对话记录等。数据的质量和多样性对于训练出高质量的LLM非常重要。
预处理：对数据进行预处理以使其适合模型训练。这包括分词（将文本划分为词或子词单元）、建立词汇表（将词映射到数字表示）、清理和规范化文本等操作。
构建模型架构：选择适当的模型架构来构建LLM。目前最常用的模型架构是Transformer，其中包含多层的自注意力机制和前馈神经网络层。
预训练：使用大规模的文本数据集对模型进行预训练。预训练是指在无监督的情况下，通过让模型学习预测缺失的词语或下一个词语等任务来提取语言知识。这使得模型能够学习到丰富的语言表示。
微调（Fine-tuning）：在预训练之后，使用特定的任务数据对模型进行微调。微调是指在特定任务的标注数据上进行有监督的训练，例如文本生成、问题回答等。通过微调，模型可以更好地适应特定任务的要求。
超参数调优：调整模型的超参数，例如学习率、批量大小、模型层数等，以获得更好的性能和效果。
评估和迭代：对训练后的模型进行评估，并根据评估结果进行迭代改进。这可能包括调整模型架构、增加训练数据、调整训练策略等。</description></item><item><title>Reinforcement Learning from Human Feedback</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/RLHF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/RLHF/</guid><description>Review: Reinforcement Learning Basics Reinforcement learning is a mathematical framework.
Demystify the reinforcement learning model, it&amp;rsquo;s a open-ended model using reward function to optimize agent to solve complex task in target environment.</description></item><item><title>Tasks to evaluate BERT - Maybe can be deployed in other LM</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/</guid><description>Overview MNLI-m (Multi-Genre Natural Language Inference - Matched): MNLI-m is a benchmark dataset and task for natural language inference (NLI).</description></item><item><title>Train LLM from scratch</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM/</guid><description>Find a dataset Find a corpus of text in language you prefer.
Such as OSCAR Intuitively, the more data you can get to pretrain on, the better results you will get.</description></item></channel></rss>