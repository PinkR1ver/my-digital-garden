<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep-learning on</title><link>https://pinktalk.online/tags/deep-learning/</link><description>Recent content in deep-learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://pinktalk.online/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>AdaBoost</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/AdaBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/AdaBoost/</guid><description>Video you need to watch first AdaBoost, Clearly Explained Key words and equation Stump(树桩) means classification just by one feature Amount of say $$ \text{Amout of say} = \frac{1}{2}\log{(\frac{1-\text{Total Error}}{\text{Total Error}})} $$</description></item><item><title>Deep Learning - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep-_Learning_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep-_Learning_MOC/</guid><description>Deep Learning Block &amp;amp; Machine Learning - MOC
Model Interpretability
Famous Model - MOC</description></item><item><title>Deep Neural Decision Forests</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Neural_Decision_Forests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Deep_Neural_Decision_Forests/</guid><description>Background Decision Tree Random Forest What is Deep Neural Decision Forests Deep Neural Decision Forests(dNDFs)是Neural Networks和Random Forest的结合，但是它更倾向于Neural Networks。它本质上是Nerual Networks incorporate Random Forest来提高NN的效率和准确度，训练方法和NN一致。</description></item><item><title>DeepAR - Time Series Forcasting</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/DeepAR/</guid><description>DeepAR, an autoregressive recurrent network developed by Amazon, is the first model that could natively work on multiple time-series. It&amp;rsquo;s a milestone in time-series community.</description></item><item><title>Famous Model MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Famous_Model_MOC/</guid><description>Time-series DeepAR</description></item><item><title>Gated Recurrent Unit</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/GRU/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/GRU/</guid><description/></item><item><title>How to do use fine tune tech to create your chatbot</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/how_to_fine_tune/</guid><description/></item><item><title>How to make custom dataset?</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/dataset/make_custom_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/dataset/make_custom_dataset/</guid><description/></item><item><title>Large Language Model(LLM) - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/LLM_MOC/</guid><description>Training Training Tech Outline ⭐⭐⭐Train LLM from scratch ⭐⭐⭐Detailed explanation of RLHF technology How to do use fine tune tech to create your chatbot Learn finetune by Stanford Alpaca Metrics How to evaluate a LLM performance?</description></item><item><title>Learn finetune by Stanford Alpaca</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/finr_tune/learn_finetune_byStanfordAlpaca/</guid><description> Reference https://www.youtube.com/watch?v=pcszoCYw3vc https://crfm.stanford.edu/2023/03/13/alpaca.html</description></item><item><title>LLM hyperparameter</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/basic/llm_hyperparameter/</guid><description>LLM Temperature Temperature definition come from the physical meaning of temperature. The more higher temperature, the atoms moving more faster, meaning more randomness.</description></item><item><title>LLM training steps</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/steps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/steps/</guid><description>训练大型语言模型（LLM）的方法通常涉及以下步骤：
数据收集：收集大规模的文本数据作为训练数据。这些数据可以是互联网上的文本、书籍、文章、新闻、对话记录等。数据的质量和多样性对于训练出高质量的LLM非常重要。
预处理：对数据进行预处理以使其适合模型训练。这包括分词（将文本划分为词或子词单元）、建立词汇表（将词映射到数字表示）、清理和规范化文本等操作。
构建模型架构：选择适当的模型架构来构建LLM。目前最常用的模型架构是Transformer，其中包含多层的自注意力机制和前馈神经网络层。
预训练：使用大规模的文本数据集对模型进行预训练。预训练是指在无监督的情况下，通过让模型学习预测缺失的词语或下一个词语等任务来提取语言知识。这使得模型能够学习到丰富的语言表示。
微调（Fine-tuning）：在预训练之后，使用特定的任务数据对模型进行微调。微调是指在特定任务的标注数据上进行有监督的训练，例如文本生成、问题回答等。通过微调，模型可以更好地适应特定任务的要求。
超参数调优：调整模型的超参数，例如学习率、批量大小、模型层数等，以获得更好的性能和效果。
评估和迭代：对训练后的模型进行评估，并根据评估结果进行迭代改进。这可能包括调整模型架构、增加训练数据、调整训练策略等。</description></item><item><title>Long Short-Term Memory Networks</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/LSTM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/LSTM/</guid><description>[!quote] When I was learning LSTM, the new deep learning block Transformers dominate the NLP field. However, Transformers don&amp;rsquo;t decisively outperform LSTMS in time-series-related tasks.</description></item><item><title>Model Evaluation - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/model_evaluation_MOC/</guid><description> Model Evaluation in Time Series Forecasting</description></item><item><title>Model Evaluation in Time Series Forecasting</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Evaluation/time_series_forecasting/</guid><description>Some famous time series scoring technics MAE, RMSE and AIC Mean Forecast Accuracy Warning: The time series model EVALUATION TRAP!</description></item><item><title>Model Interpretability - MOC</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/Model_Interpretability_MOC/</guid><description> SHAP</description></item><item><title>Quantile loss</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Trick/quantile_loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Trick/quantile_loss/</guid><description>在大多数现实世界的预测问题中，我们的预测所带来的不确定性具有重要价值。相较于仅仅提供点估计，了解预测范围能够显著改善许多商业应用的决策过程。Quantile loss就是为例帮助我们了解预测范围的loss function。
Quantile loss用于衡量预测分布和目标分布之间的差异，特别适用于处理不确定性较高的预测问题。
What is quantile Quantile
What is a prediction interval 预测区间是对预测的不确定性进行量化的一种方法。它为结果变量的估计提供了概率上限和下限的范围。
输出本身是随机变量，因此具有分布特性。预测区间的目的在于了解结果的正确性可能性。
What is Quantile Loss 在Quantile loss中，我们将预测结果和目标值都表示为分位数形式，例如，我们可以用预测的α分位数来表示预测结果，用真实值的α分位数来表示目标值。然后，Quantile loss衡量了这两个分布之间的差异，通常使用分位数损失函数来计算。</description></item><item><title>Reinforcement Learning from Human Feedback</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/RLHF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/RLHF/</guid><description>Review: Reinforcement Learning Basics Reinforcement learning is a mathematical framework.
Demystify the reinforcement learning model, it&amp;rsquo;s a open-ended model using reward function to optimize agent to solve complex task in target environment.</description></item><item><title>SHAP - a reliable way to analyze model interpretability</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Model_interpretability/SHAP/</guid><description>SHAP is the most popular model-agnostic technique that is used to explain predictions. SHAP stands for SHapley Additive exPlanations
Shapely values are obtained by incorporating concepts from Cooperative Game Theory and local explanations</description></item><item><title>Tasks to evaluate BERT - Maybe can be deployed in other LM</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/metircs/some_task/</guid><description>Overview MNLI-m (Multi-Genre Natural Language Inference - Matched): MNLI-m is a benchmark dataset and task for natural language inference (NLI).</description></item><item><title>Temporal Fusion Transformer</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Temporal_Fusion_Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Famous_Model/Temporal_Fusion_Transformer/</guid><description/></item><item><title>Tokenization</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/NLP/basic/tokenization/</guid><description/></item><item><title>Train LLM from scratch</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/LLM/train/train_LLM/</guid><description>Find a dataset Find a corpus of text in language you prefer.
Such as OSCAR Intuitively, the more data you can get to pretrain on, the better results you will get.</description></item><item><title>Transformer</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Transformer/</guid><description> [!info] 在学习Transformer前，你需要学习 ⭐Attention
Transformer 是Seq2Seq model，由Encoder和Decoder组成 Encoder 这里贴的是原文Encoder的架构</description></item><item><title>XGBoost</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/XGBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/XGBoost/</guid><description>XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the Gradient Boosting framework.</description></item><item><title>⭐Attenion</title><link>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep_Learning_And_Machine_Learning/Deep_Learning_Block_and_Machine_Learning_Block/Attention/</guid><description>Self-Attention 讲述self-attention我们以sequence labeling任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。
典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。
我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助self-attention层。
Detail 对于Self-attention层，生成的$b^i$向量是考虑到所有输入$\sum_i\alpha^i$向量
Vector Relevance Step 1. 使用Dot-product 去计算 vector relevance Step 2.</description></item></channel></rss>