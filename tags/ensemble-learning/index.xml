<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ensemble-learning on</title><link>https://pinktalk.online/tags/ensemble-learning/</link><description>Recent content in ensemble-learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://pinktalk.online/tags/ensemble-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>AdaBoost</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/AdaBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/AdaBoost/</guid><description>Video you need to watch first AdaBoost, Clearly Explained Key words and equation Stump(树桩) means classification just by one feature Amount of say $$ \text{Amout of say} = \frac{1}{2}\log{(\frac{1-\text{Total Error}}{\text{Total Error}})} $$</description></item><item><title>XGBoost</title><link>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/XGBoost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/computer_sci/Deep_Learning_And_Machine_Learning/deep_learning/XGBoost/</guid><description>XGBoost is an open-source software library that implements optimized distributed gradient boosting machine learning algorithms under the Gradient Boosting framework.</description></item></channel></rss>