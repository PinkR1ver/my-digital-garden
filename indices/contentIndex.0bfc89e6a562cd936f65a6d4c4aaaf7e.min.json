{"/":{"title":"MOC","content":"\n[[Deep Learning And Machine Learning/Deep Learning Block/Deep Learning Block - Table of Content|Deep Learning Block - MOC]]\n\n[Signal Processing - MOC](Signal%20Processing/Signal%20Processing_MOC.md)\n\n[[Synthetic Aperture Radar Imaging/SAR_MOC| Synthetic Aperture Radar(SAR) Imaging - MOC]]","lastmodified":"2023-03-20T09:15:12.031570912Z","tags":null},"/.trash/attachments/Pasted-image-20230320150424.png":{"title":"Pasted image 20230320150424.png","content":"","lastmodified":"2023-03-20T09:15:11.975570314Z","tags":null},"/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Attention":{"title":"⭐Attenion","content":"# Self-Attention\n\n讲述self-attention我们以*sequence labeling*任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。\n\n典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。\n\n![Pasted image 20230315195403](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/1.png)\n\n我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助**self-attention**层。\n\n## Detail\n\n![Pasted image 20230315195603](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315195603.png)\n\n对于Self-attention层，生成的$b^i$向量是考虑到所有输入$\\sum_i\\alpha^i$向量\n\n### Vector Relevance\n\n![250](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315200009.png)\n\n\n* *Step 1.* 使用Dot-product 去计算 vector relevance\n\n![400](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315201906.png)\n\n* *Step 2.* Normalizing计算出来的vector relevance\n![400](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315202047.png)\n\n* *Step 3.*  根据vector relevance，也就是attention scores计算最后的输出。这是一个Reweighting Process，一个extract information based on attention scores\n\n![400](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315202314.png)\n\n\u003e [!hint] \n\u003e  从上面的过程中，可以看出，$b^i$互相之间的计算没有关系，具有很好的并行性\n\n### Matrix Detail\n\n$$\nq^i = W^q \\alpha^i\n$$\n\n\n$$\nQ = [q^1 \\quad q^2 \\quad \\cdots \\quad q^N],\\ \\  I = [\\alpha^1 \\quad \\alpha^2 \\quad \\cdots \\quad \\alpha^N]\n$$\n\n\n\nSo,\n\n$$\nQ = W^q I\n$$\n\nAs same,\n$$\nK = W^k I,\\quad V = W^v I\n$$\nCalculate attention score $\\alpha$,\n$$\n\\begin{bmatrix}\n\\alpha_{1,1} \\\\\n\\alpha_{1,2} \\\\\n\\cdots \\\\\n\\alpha_{1,N}\n\\end{bmatrix} =\n\\begin{bmatrix}\nk^1 \\\\\nk^2 \\\\\n\\cdots \\\\\nk_N\n\\end{bmatrix} q^1\n$$\n\nSo,\n$$\nA=\\begin{bmatrix}\n\\alpha_{1,1} \u0026 \\alpha_{2,1} \u0026 \\cdots \u0026 \\alpha_{N,1} \\\\\n\\alpha_{1,2} \u0026 \\alpha_{2,2} \u0026 \\cdots \u0026 \\alpha_{N,2} \\\\\n\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots\\\\\n\\alpha_{1,N} \u0026 \\alpha_{2,N} \u0026 \\cdots \u0026 \\alpha_{N,N}\n\\end{bmatrix} =\n\\begin{bmatrix}\nk^1 \\\\\nk^2 \\\\\n\\cdots \\\\\nk_N\n\\end{bmatrix} [q^1 \\quad q^2 \\quad \\cdots \\quad q^N] = K^TQ\n$$\n\n$$\nA' = \\text{Softmax}(A)\n$$\n\nFinally, calculate output $b$\n\n$$\nO = [b^1 \\quad b^2 \\quad \\cdots \\quad b^N] = [v^1 \\quad v^2 \\quad \\cdots \\quad v^N] = VA'\n$$\n\n![600](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315205148.png)\n\n### Positional Encoding\n![250](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315205727.png)\n* Each position has a unique positional vector $e^i$\n\t* hand-crafted\n\t* learned from data\n\n## Fun Facts\n\n### Self-attention vs. CNN\n\n![Pasted image 20230315205918](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315205918.png)\n\n因为transformer有着更大的function set，所以需求更多的数据; ![Pasted image 20230315210032](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315210032.png)\n\n### Self-attention vs. RNN\n\n目前，RNN的角色正在被self-attention替代，RNN在long seq的情况下，前面的信息会被逐渐遗忘；同时**RNN没有并行性**\n同样，Self attention有着比RNN更大的function set，在某些情况下，self-attention可以变成RNN\n\n# Multi-head Self-attention\nMulti-head self attention就是由不同的self attention layer在一起，有不同的$W^q$,$W^k$来负责不同种类的relevance\n\n![600](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315210631.png)\n![300](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230315210704.png) ","lastmodified":"2023-03-20T09:15:12.027570869Z","tags":null},"/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Deep-Learning-Block-Table-of-Content":{"title":"Deep Learning Block - Table of Content","content":"\n[[Deep Learning And Machine Learning/Deep Learning Block/⭐Attention|Attention Blocker]]\n\n[[Deep Learning And Machine Learning/Deep Learning Block/Transformer|Transformer]]\n\n","lastmodified":"2023-03-20T09:15:11.975570314Z","tags":null},"/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Transformer":{"title":"Transformer","content":"\n\u003e [!info] \n\u003e 在学习Transformer前，你需要学习 [⭐Attention](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/⭐Attention.md)\n\n\n\nTransformer 是Seq2Seq model，由Encoder和Decoder组成\n![300](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230316160103.png)\n\n# Encoder\n这里贴的是原文Encoder的架构\n![Pasted image 20230316162635](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230316162635.png)\n\n![Pasted image 20230316162642](Deep%20Learning%20And%20Machine%20Learning/Deep%20Learning%20Block/attachments/Pasted%20image%2020230316162642.png)","lastmodified":"2023-03-20T09:15:11.975570314Z","tags":null},"/Signal-Processing/Basic-Concepts-in-Signal-Processing":{"title":"Basic Concepts in Signal Processing","content":"\n[What is dB](Signal%20Processing/What%20is%20dB.md)","lastmodified":"2023-03-20T09:15:12.027570869Z","tags":null},"/Signal-Processing/Signal-Processing_MOC":{"title":"Signal Processing - MOC","content":"\n[Basic Concepts in Signal Processing](Signal%20Processing/Basic%20Concepts%20in%20Signal%20Processing.md)\n","lastmodified":"2023-03-20T09:15:12.027570869Z","tags":null},"/Signal-Processing/What-is-dB":{"title":"What is dB","content":"dB is short for decibel, which is a unit that indicates ratio or gain. It is often used to measure *sound intensity*, *signal strength*, *attenuation* and other quantities. \n\nFor example, if a sound has a power of 10 W and another sound has a power of 1 W, then the difference in decibels is 10 dB = 10 log (10/1) = 10 log 10 = 10.\n\n**Signal Noise Ratio** is also measured by dB\n\n## Signal Noise Ratio\n$$\n{SNR}_{power}=\\frac{\\text{Average Signal Power}}{\\text{Average Noise Power}}\n$$\n\n$$\n{SNR}_{voltage}=\\frac{\\text{RMS Signal Voltage}}{\\text{RMS Noise Voltage}}\n$$\n\n$$\n{SNR}_{power}={{SNR}_{voltage}}^2\n$$\n\n$$\n{SNR}_{dB}=10\\log_{10}{{SNR}_{power}}=20\\log_{10}{{SNR}_{voltage}}\n$$\n","lastmodified":"2023-03-20T09:15:12.027570869Z","tags":null},"/Synthetic-Aperture-Radar-Imaging/SAR-Explained":{"title":"Synthetic Aperture Radar (SAR) Explained","content":"\n# Radar Basic Concepts\n\n## Down Looking vs. Side Looking\n\n![Pasted image 20230320150424](Synthetic%20Aperture%20Radar%20Imaging/attachments/Pasted%20image%2020230320150424.png)\n\nDown Looking不能区分距离一样的a，b点\n\n## Simplified explanation of Radar working \u0026 What is SAR\nThe radar consists fundamentally of *a transmitter*, *a receiver*, *an antenna* and *an electronic system* to process and record the data.\n\nThe transmitter generates successive short bursts or pulses of microwave at regular intervals which are focused by the antenna into a beam. Radar beam illuminates the surface **obliquely** at a right angle to the motion of the platform. The antenna receives a portion of the transmitted energy reflected or it's known as backscattered from various objects within the illuminated beam by  measuring this time delay between the transmission of a pulse and the reception of the backscattered echo from different  targets. Their distance from the radar and therefore their location can be determined as the sensor platform moves forward recording and processing of the backscattered signals builds up a 2-dimensional image of the surface.\n\n\n\u003e [!important] \n\u003e The along track **resolution** is determined by the beam width which is *inversely proportional to the antenna length*, also known as the **aperture**, which means that longer antenna or a longer aperture will produce a narrow beam and a finer resolution.\n\u003e \n\u003e Long antenna $\\leftrightarrow$ Small beam $\\leftrightarrow$ Long aperture $\\leftrightarrow$ Better image resolution\n\n介于实际情况下的物理空间中，雷达天线的大小是限的，可以通过雷达的移动去模拟长天线情况下的雷达，也就是活得更大的aperture，这项被叫做SAR。目的是在于使用*comparatively small physical antennas*去获得*high resolution images*\n\n## Review of Radar Image Formation\n\n![660](Synthetic%20Aperture%20Radar%20Imaging/attachments/Pasted%20image%2020230320163240.png)\n\n* Radar can measure *amplitude* and *phase*\n* Radar can only measure part of echoes.\n* The strength of the reflected echo is the backscattering coefficient (sigma naught) and is expressed in [decibels(dB)](Signal%20Processing/What%20is%20dB.md)","lastmodified":"2023-03-20T09:15:12.027570869Z","tags":null},"/Synthetic-Aperture-Radar-Imaging/SAR_MOC":{"title":"Synthetic Aperture Radar (SAR) Imaging - MOC","content":"\n[[Synthetic Aperture Radar Imaging/SAR Explained|SAR Explained]]\n","lastmodified":"2023-03-20T09:15:12.027570869Z","tags":null}}