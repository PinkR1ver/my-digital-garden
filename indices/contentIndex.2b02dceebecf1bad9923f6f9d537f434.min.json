{"/":{"title":"_index","content":"[[Deep Learning \u0026 Machine Learning/Deep Learning Block/Deep Learning Block - Table of Content|Deep Learning Block - Table of Content]]\n","lastmodified":"2023-03-16T13:50:44.599088899Z","tags":null},"/Deep-Learning-Machine-Learning/Deep-Learning-Block/Attention":{"title":"⭐Attention","content":"\n# Self-Attention\n\n讲述self-attention我们以*sequence labeling*任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。\n\n典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315195403.png]]\n\n我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助**self-attention**层。\n\n## Detail\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315195603.png]]\n\n对于Self-attention层，生成的$b^i$向量是考虑到所有输入$\\sum_i\\alpha^i$向量\n\n### Vector Relevance\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315200009.png|250]]\n\n\n* *Step 1.* 使用Dot-product 去计算 vector relevance\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315201906.png|400]]\n\n* *Step 2.* Normalizing计算出来的vector relevance\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315202047.png|400]]\n\n* *Step 3.*  根据vector relevance，也就是attention scores计算最后的输出。这是一个Reweighting Process，一个extract information based on attention scores\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315202314.png|400]]\n\n\u003e [!hint] \n\u003e  从上面的过程中，可以看出，$b^i$互相之间的计算没有关系，具有很好的并行性\n\n### Matrix Detail\n\n$$\nq^i = W^q \\alpha^i\n$$\n\n\n$$\nQ = [q^1 \\quad q^2 \\quad \\cdots \\quad q^N],\\ \\  I = [\\alpha^1 \\quad \\alpha^2 \\quad \\cdots \\quad \\alpha^N]\n$$\n\n\n\nSo,\n\n$$\nQ = W^q I\n$$\n\nAs same,\n$$\nK = W^k I,\\quad V = W^v I\n$$\nCalculate attention score $\\alpha$,\n$$\n\\begin{bmatrix}\n\\alpha_{1,1} \\\\\n\\alpha_{1,2} \\\\\n\\cdots \\\\\n\\alpha_{1,N}\n\\end{bmatrix} =\n\\begin{bmatrix}\nk^1 \\\\\nk^2 \\\\\n\\cdots \\\\\nk_N\n\\end{bmatrix} q^1\n$$\n\nSo,\n$$\nA=\\begin{bmatrix}\n\\alpha_{1,1} \u0026 \\alpha_{2,1} \u0026 \\cdots \u0026 \\alpha_{N,1} \\\\\n\\alpha_{1,2} \u0026 \\alpha_{2,2} \u0026 \\cdots \u0026 \\alpha_{N,2} \\\\\n\\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots\\\\\n\\alpha_{1,N} \u0026 \\alpha_{2,N} \u0026 \\cdots \u0026 \\alpha_{N,N}\n\\end{bmatrix} =\n\\begin{bmatrix}\nk^1 \\\\\nk^2 \\\\\n\\cdots \\\\\nk_N\n\\end{bmatrix} [q^1 \\quad q^2 \\quad \\cdots \\quad q^N] = K^TQ\n$$\n\n$$\nA' = \\text{Softmax}(A)\n$$\n\nFinally, calculate output $b$\n\n$$\nO = [b^1 \\quad b^2 \\quad \\cdots \\quad b^N] = [v^1 \\quad v^2 \\quad \\cdots \\quad v^N] = VA'\n$$\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315205148.png|600]]\n\n### Positional Encoding\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315205727.png|250]]\n* Each position has a unique positional vector $e^i$\n\t* hand-crafted\n\t* learned from data\n\n## Fun Facts\n\n### Self-attention vs. CNN\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315205918.png]]\n\n因为transformer有着更大的function set，所以需求更多的数据; ![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315210032.png]]\n\n### Self-attention vs. RNN\n\n目前，RNN的角色正在被self-attention替代，RNN在long seq的情况下，前面的信息会被逐渐遗忘；同时**RNN没有并行性**\n同样，Self attention有着比RNN更大的function set，在某些情况下，self-attention可以变成RNN\n\n# Multi-head Self-attention\nMulti-head self attention就是由不同的self attention layer在一起，有不同的$W^q$,$W^k$来负责不同种类的relevance\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315210631.png|600]]\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230315210704.png|300]] ","lastmodified":"2023-03-16T13:50:44.599088899Z","tags":null},"/Deep-Learning-Machine-Learning/Deep-Learning-Block/Deep-Learning-Block-Table-of-Content":{"title":"Deep Learning Block - Table of Content","content":"[[⭐Attention | Attention Blocker]]\n\n[[Transformer | Transformer]]\n\n","lastmodified":"2023-03-16T13:50:44.547088309Z","tags":null},"/Deep-Learning-Machine-Learning/Deep-Learning-Block/Transformer":{"title":"Transformer","content":"\u003e [!info] \n\u003e 在学习Transformer前，你需要学习 [[Deep Learning \u0026 Machine Learning/Deep Learning Block/⭐Attention]]\n\n\n\nTransformer 是Seq2Seq model，由Encoder和Decoder组成\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230316160103.png|300]]\n\n# Encoder\n这里贴的是原文Encoder的架构\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230316162635.png]]\n\n![[Deep Learning \u0026 Machine Learning/Deep Learning Block/attachments/Pasted image 20230316162642.png]]","lastmodified":"2023-03-16T13:50:44.547088309Z","tags":null}}