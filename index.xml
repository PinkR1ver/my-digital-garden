<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MOC on</title><link>https://pinktalk.online/</link><description>Recent content in MOC on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://pinktalk.online/index.xml" rel="self" type="application/rss+xml"/><item><title>Basic Concepts in Signal Processing</title><link>https://pinktalk.online/Signal-Processing/Basic-Concepts-in-Signal-Processing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Signal-Processing/Basic-Concepts-in-Signal-Processing/</guid><description>What is dB</description></item><item><title>Deep Learning Block - MOC</title><link>https://pinktalk.online/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Deep-Learning-Block/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Deep-Learning-Block/</guid><description>[[Deep Learning And Machine Learning/Deep Learning Block/⭐Attention|Attention Blocker]]
[[Deep Learning And Machine Learning/Deep Learning Block/Transformer|Transformer]]</description></item><item><title>Photography - MOC</title><link>https://pinktalk.online/Photography/Photography_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Photography/Photography_MOC/</guid><description>About Basic Concepts:
Saturation</description></item><item><title>Saturation - 饱和度</title><link>https://pinktalk.online/Photography/Saturation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Photography/Saturation/</guid><description>to be written&amp;hellip;</description></item><item><title>Signal Processing - MOC</title><link>https://pinktalk.online/Signal-Processing/Signal-Processing_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Signal-Processing/Signal-Processing_MOC/</guid><description>Basic Concepts in Signal Processing</description></item><item><title>Synthetic Aperture Radar (SAR) Explained</title><link>https://pinktalk.online/Synthetic-Aperture-Radar-Imaging/SAR-Explained/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Synthetic-Aperture-Radar-Imaging/SAR-Explained/</guid><description>Radar Basic Concepts Down Looking vs. Side Looking Down Looking不能区分距离一样的a，b点
Simplified explanation of Radar working &amp;amp; What is SAR The radar consists fundamentally of a transmitter, a receiver, an antenna and an electronic system to process and record the data.</description></item><item><title>Synthetic Aperture Radar (SAR) Imaging - MOC</title><link>https://pinktalk.online/Synthetic-Aperture-Radar-Imaging/SAR_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Synthetic-Aperture-Radar-Imaging/SAR_MOC/</guid><description>[[Synthetic Aperture Radar Imaging/SAR Explained|SAR Explained]]</description></item><item><title>Transformer</title><link>https://pinktalk.online/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Transformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Transformer/</guid><description> [!info] 在学习Transformer前，你需要学习 ⭐Attention
Transformer 是Seq2Seq model，由Encoder和Decoder组成 Encoder 这里贴的是原文Encoder的架构</description></item><item><title>What is dB</title><link>https://pinktalk.online/Signal-Processing/What-is-dB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Signal-Processing/What-is-dB/</guid><description>dB is short for decibel, which is a unit that indicates ratio or gain. It is often used to measure sound intensity, signal strength, attenuation and other quantities.</description></item><item><title>⭐Attenion</title><link>https://pinktalk.online/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/Deep-Learning-And-Machine-Learning/Deep-Learning-Block/Attention/</guid><description>Self-Attention 讲述self-attention我们以sequence labeling任务作为任务来讲解，sequence labeling的任务是输入N个vector并且输出N个label。
典型的例子有输入一个句子，分析每个词汇的词性是什么，比如句子“I saw a saw”，这个句子里saw和saw的词性分别是verb和nonu，如果我们用fully-connected（FC）层来做的话，那么面对同样的输入saw，我们无法得出不同的结果。
我们的做法可以是对输入加窗，考虑周边邻近的词汇信息，这与信号处理常用的方法类似，但是窗的长度是有限且固定的，而seq的长度是变化的，因此我们在面对这种任务的时候，我们可以借助self-attention层。
Detail 对于Self-attention层，生成的$b^i$向量是考虑到所有输入$\sum_i\alpha^i$向量
Vector Relevance Step 1. 使用Dot-product 去计算 vector relevance Step 2.</description></item><item><title>句子</title><link>https://pinktalk.online/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/%E6%96%87%E5%AD%A6/%E5%8F%A5%E5%AD%90/</guid><description>[!quote] No easy basket
“如果你想了解American篮球的根基，你要去看看美高”
[!quote] A poem 至少有两次喜欢
一次发生在在一起之前，一次发生在之后，
第一次是喜欢上你。第二次是喜欢上我们，
我只敢把第二次翻译成爱，
第一次是因为你很好，第二次是因为我还没有坏到敢放满揉碎你的好。
我要小心的捧着第一次的喜欢，就像掏出一份手写的初稿，
你会接过我的目光，在岁月里重新誊写岁月，
要删改的地方还很多，包括但不限于，把差错翻译成幽默</description></item><item><title>文学</title><link>https://pinktalk.online/%E6%96%87%E5%AD%A6/%E6%96%87%E5%AD%A6_MOC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pinktalk.online/%E6%96%87%E5%AD%A6/%E6%96%87%E5%AD%A6_MOC/</guid><description>In this MOC, it shows you the path to what I record for some interesting sentences, including Chinese and English, even Japanese.</description></item></channel></rss>